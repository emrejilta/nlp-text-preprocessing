{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing with spaCy"
      ],
      "metadata": {
        "id": "aDr_g3TkwyBa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "80iqirTjB8Ld"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The corpus is considered to be located on Google Drive"
      ],
      "metadata": {
        "id": "H9DyDkDBxM-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"drive/MyDrive/datasets/hamlet.txt\")\n",
        "text = file.read()"
      ],
      "metadata": {
        "id": "oM55MBHsCJ4L"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The English pipeline is required in order to use the spaCy correctly."
      ],
      "metadata": {
        "id": "GrRFC5QJxYJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "gMot6BLAC9k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(name='en_core_web_sm')"
      ],
      "metadata": {
        "id": "xXaY0hY9ER6S"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "H7ENqSEBxgZ_"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segmentation"
      ],
      "metadata": {
        "id": "8nQKrQUIxvfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in doc.sents:\n",
        "    print(sentence.text)"
      ],
      "metadata": {
        "id": "IReN772gDIXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "dH6REE-ZxyyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenization = nlp(text)\n",
        "for token in tokenization:\n",
        "  print(token)"
      ],
      "metadata": {
        "id": "mSmSM4fTEIPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "rGlWJJJIx2jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatization = nlp(text)\n",
        "for token in lemmatization:\n",
        "  print(token.text,'\\t',token.lemma_)"
      ],
      "metadata": {
        "id": "TG4_dlolGldG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords Removal"
      ],
      "metadata": {
        "id": "rSpPgKVmx4ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = nlp.Defaults.stop_words\n",
        "with_st = []\n",
        "without_st = []\n",
        "\n",
        "for token in doc:\n",
        "    if token.is_stop:\n",
        "      with_st.append(token.text)\n",
        "    else:\n",
        "      without_st.append(token.text)\n",
        "\n",
        "print(\"With stopwords\")\n",
        "print(with_st)\n",
        "\n",
        "print(\"Without stopwords\")\n",
        "print(without_st)"
      ],
      "metadata": {
        "id": "2Ki7WeKHUgPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part-of-Speech"
      ],
      "metadata": {
        "id": "Rrbvkof6x7xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in doc:\n",
        "    print(word.text,  word.pos_)"
      ],
      "metadata": {
        "id": "Gq5HeYb2d9R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing Punctutation"
      ],
      "metadata": {
        "id": "iZRwczcnx_i5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_r = text.replace(\"\\n\", \" \").strip()\n",
        "doc_r = nlp(text_r)\n",
        "\n",
        "n_p_list = []\n",
        "for token in doc_r:\n",
        "  if not token.is_punct:\n",
        "    n_p_list.append(token)\n",
        "\n",
        "n_p = []\n",
        "for token in n_p_list:\n",
        "  n_p.append(token.text.lower())\n",
        "\n",
        "print(n_p)"
      ],
      "metadata": {
        "id": "_3A_fkigi_J8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
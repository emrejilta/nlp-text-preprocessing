{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing with NLTK"
      ],
      "metadata": {
        "id": "pFGzaHh2yTfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apart from itself, also the other modules are required."
      ],
      "metadata": {
        "id": "mmfhDXcbycgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import sent_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "eV8IwvwY6xU4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use the modules above, below are required to be downloaded as well."
      ],
      "metadata": {
        "id": "nG7A5stGyrvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "t_pTYVXS8ALY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The corpus is considered to be located on Google Drive"
      ],
      "metadata": {
        "id": "iQisSgsiy_kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"drive/MyDrive/datasets/hamlet.txt\")\n",
        "\n",
        "text = file.read()"
      ],
      "metadata": {
        "id": "lJvE9riV7PsQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segmentation"
      ],
      "metadata": {
        "id": "SjI3rqhGzBAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Segmentation - Cümlelere Ayırma\n",
        "print(\"Sentence Segmentation\")\n",
        "sentences = sent_tokenize(text)\n",
        "f1 = open(\"sentence.txt\", \"w\")\n",
        "for s in sentences:\n",
        "    print(s)\n",
        "    f1.write(s)\n",
        "f1.close()"
      ],
      "metadata": {
        "id": "Ww14JFfw7cyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "r-_uSqzlzCsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization - Sözcüklere Ayırma\n",
        "print(\"Tokenization\")\n",
        "words = word_tokenize(text)\n",
        "f2 = open(\"tokenization.txt\", \"w\")\n",
        "for w in words:\n",
        "    print(w)\n",
        "    f2.write(\"\".join(w)+\"\\n\")\n",
        "f2.close()"
      ],
      "metadata": {
        "id": "sUIl7gAv9REb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "OgfHJMC5zEh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming - Kök İndirgeme\n",
        "print(\"Stemming\")\n",
        "ps = PorterStemmer()\n",
        "ss = SnowballStemmer(\"english\")\n",
        "f3a = open(\"porter-stemming.txt\", \"w\")\n",
        "f3b = open(\"snowball-stemming.txt\", \"w\")\n",
        "for w in words:\n",
        "    psresult = w + \" : \" + ps.stem(w) + \"\\n\"\n",
        "    ssresult = w + \" : \" + ss.stem(w) + \"\\n\"\n",
        "    print(\"Porter Stemmer\")\n",
        "    print(psresult)\n",
        "    print(\"Snowball Stemmer\")\n",
        "    print(ssresult)\n",
        "    f3a.write(psresult)\n",
        "    f3b.write(ssresult)\n",
        "f3a.close()\n",
        "f3b.close()"
      ],
      "metadata": {
        "id": "iolTtrt29zQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "YAV7Alg4zGIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lemmatization\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "f4 = open(\"lemmatization.txt\", \"w\")\n",
        "for w in words:\n",
        "    lem = w + \" : \" + lemmatizer.lemmatize(w)\n",
        "    print(lem)\n",
        "    f4.write(lem)\n",
        "f4.close()"
      ],
      "metadata": {
        "id": "zVzzLom8_-DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords"
      ],
      "metadata": {
        "id": "vD4V5s1OzHu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Stopwords\")\n",
        "stop_words = stopwords.words('english')\n",
        "f5a = open(\"non-stopwords.txt\", \"w\")\n",
        "f5b = open(\"stopwords.txt\", \"w\")\n",
        "\n",
        "for r in words:\n",
        "    if not r in stop_words:\n",
        "        nost = r + \"\\n\"\n",
        "        print(nost)\n",
        "        f5a.write(nost)\n",
        "    else:\n",
        "        st = r + \"\\n\"\n",
        "        print(st)\n",
        "        f5b.write(st)\n",
        "f5a.close()\n",
        "f5b.close()"
      ],
      "metadata": {
        "id": "cOH8EerPAmSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part-of-Speech"
      ],
      "metadata": {
        "id": "6PbzWdYpzJzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Part-of-speech - Sözcük Türü Etiketleme\n",
        "print(\"Part-of-speech\")\n",
        "f6 = open(\"part-of-speech.txt\", \"w\")\n",
        "for i in sentences:\n",
        "    wordsList = nltk.word_tokenize(i)\n",
        "\n",
        "    # removing stop words from wordList\n",
        "    wordsList = [w for w in wordsList if not w in stop_words]\n",
        "\n",
        "    #  Using a Tagger. Which is part-of-speech\n",
        "    # tagger or POS-tagger.\n",
        "    tagged = nltk.pos_tag(wordsList)\n",
        "\n",
        "    for t in tagged:\n",
        "        pprint(t)\n",
        "        f6.write(\"\".join(f'{t[0]} \\t {t[1]}')+\"\\n\")\n",
        "f6.close()"
      ],
      "metadata": {
        "id": "md_ymu25IzIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing Punctuation"
      ],
      "metadata": {
        "id": "etyM6pe3zNoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Punctuation - Noktalama İşaretini Kaldırma\n",
        "print(\"Removing Punctuation\")\n",
        "tokens = nltk.wordpunct_tokenize(text)\n",
        "txt = nltk.Text(tokens)\n",
        "f7 = open(\"removing-punctuation.txt\", \"w\")\n",
        "for w in txt:\n",
        "    if w.isalpha():\n",
        "        print(w.lower())\n",
        "        f7.write(\"\".join(w.lower()) + \"\\n\")\n",
        "f7.close()"
      ],
      "metadata": {
        "id": "5I_9-yWHPoiO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}